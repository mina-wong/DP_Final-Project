{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes raw data from json file and extracts family Number, application number & date, publication number & date, patent name & abstract, applicnat & inventor name, IPC & CPC classes, family members, number of geographical extensions, and oldest and newest family publication. Processed data is returned in a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from IPython.display import display\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets region. Use country code as specified in [ESpacenet](https://worldwide.espacenet.com/patent/help/countrycodes). Country code will automatically be added to exported files' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "startYear = 2017\n",
    "endYear = 2023\n",
    "region = \"TN\"\n",
    "\n",
    "endYear = str(endYear)[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines functions to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "# attempts to extract a sequence of one or more consecutive letters from the beginning of the string appNum. \n",
    "# if match is found, returns the identified substring; otherwise returns None.\n",
    "# e.g. areaCodeRegex(\"EP3722443A1\") returns \"EP\"\n",
    "\n",
    "def areaCodeRegex(appNum):\n",
    "    return re.match(r\"[A-Za-z]+\",appNum).group()\n",
    "\n",
    "\n",
    "# extracts a string containing only json data (substring between the outermost curly brackets) from 'string'\n",
    "# if match is found, returns the identified substring; otherwise returns None.\n",
    "# e.g. extractJSON(\"lorem ipsum {'EP3722443A1':{'dolor':'sit'}} amet\") returns \"{'EP3722443A1':{'dolor':'sit'}}\"\n",
    "\n",
    "def extractJSON(string):\n",
    "    match = re.search(r'Content-Type: application/json(.*?)Content-Type: application/json', string, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return re.search(r'{[\\s\\S]*}', match.group(), re.DOTALL).group()\n",
    "    else:\n",
    "        return re.search(r'{[\\s\\S]*}', string, re.DOTALL).group()\n",
    "\n",
    "\n",
    "# calculates the number of months between two dates formatted as \"%Y-%m-%d\"\n",
    "# returns the difference in months\n",
    "# e.g. calculateMonthDiff(\"2022-01-15\", \"2023-07-20\") returns 18\n",
    "\n",
    "def calculateMonthDiff(start_date, end_date):\n",
    "    start = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    month_difference = (end.year - start.year) * 12 + (end.month - start.month)\n",
    "\n",
    "    return month_difference\n",
    "\n",
    "\n",
    "# removes punctuation and spaces from string. Turns all letters lowercase.\n",
    "# e.g. normaliseReference(\"Hello, World!\") returns \"helloworld\"\n",
    "\n",
    "def normaliseReference(string):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', string)\n",
    "    cleaned_text = re.sub(r'\\s+', '', cleaned_text).lower()\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# returns a number between 0-1 indicating the similarity between 2 strings. \n",
    "# {1: exactly the same, 0: completely different}\n",
    "\n",
    "def compareReferences(ref1, ref2):\n",
    "    if (ref1 == None) or (ref2 == None):\n",
    "        return 0\n",
    "    else:\n",
    "        distance = Levenshtein.distance(ref1, ref2)\n",
    "        similarity = 1 - (distance / max(len(ref1), len(ref2)))\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports data from pickle file created in `importData.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "\n",
    "# \"data\" is a list that holds all the patents\n",
    "\n",
    "data = list()\n",
    "\n",
    "with open(os.getcwd()+f'/data/{region}/rawData{region}.pickle' , 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "# set up dataframe\n",
    "\n",
    "df = pd.DataFrame(columns=[\"familyNumber\",\n",
    "                           \"applicationNumber\",\"applicationDate\",\n",
    "                           \"publicationNumber\",\"publicationDate\",\n",
    "                           \"patentName\",\"abstract\",\n",
    "                           \"applicant\",\"inventor\",\n",
    "                           \"IPC\",\"IPC_count\",\"CPC\",\"CPC_count\",\n",
    "                           \"familyMembers (appNum)\",\"geographicalExtensions\",\n",
    "                           \"familyDates_min\",\"familyDates_max\",\"timeDifference\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the following cell if loading a backup CSV file (e.g. if an error occurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "# df = pd.read_csv(os.getcwd()+f'/data/{region}/no_cit_backup_{region}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code iterates through the patents to extract the relevant information and appends it to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7\n",
    "\n",
    "# variables to track progress\n",
    "\n",
    "count = 0\n",
    "total = len(data)\n",
    "\n",
    "# prints a baseline (full) progress bar for reference\n",
    "\n",
    "print(\"|\"*100)\n",
    "\n",
    "# iterate through list \"data\"; each patent (data[q]) is represented by a dictionary containing the patent information\n",
    "\n",
    "\n",
    "for q in range(len(data)):\n",
    "    \n",
    "    # code will start appending data to the end of the dataframe\n",
    "    if q >= len(df):\n",
    "        # extract simple family number\n",
    "\n",
    "        familyNumber = data[q]['familyNumber']\n",
    "\n",
    "        # extract publication number and date\n",
    "\n",
    "        publicationNumber = data[q]['hits'][0]['fields']['publications.pn_docdb'][0]\n",
    "        \n",
    "        publicationDate, hasPubDate = \"\", False\n",
    "        \n",
    "        if data[q]['hits'][0]['fields'].get('publications.pd'):\n",
    "            publicationDate = data[q]['hits'][0]['fields']['publications.pd'][0]\n",
    "            hasPubDate = True\n",
    "        \n",
    "        # extract application number and date\n",
    "\n",
    "        applicationNumber, applicationDate = \"\", \"\"\n",
    "        \n",
    "        for app in data[q]['fields']['biblio'][0]:\n",
    "            \n",
    "            # identifies which application number in the family corresponds with the current publication \n",
    "            if data[q]['fields']['biblio'][0][app].get('pn_docdb') is not None:        \n",
    "                if [publicationNumber] in list(data[q]['fields']['biblio'][0][app]['pn_docdb'].values()):\n",
    "                    applicationNumber = app\n",
    "\n",
    "        if data[q]['hits'][0]['fields'].get('publications.app_fdate.untouched'):\n",
    "            applicationDate = data[q]['hits'][0]['fields']['publications.app_fdate.untouched'][0]\n",
    "\n",
    "\n",
    "        # extract patent name\n",
    "\n",
    "        patentName, patentAbstract = \"\", \"\"\n",
    "        hasTitle = False\n",
    "        \n",
    "        if data[q]['hits'][0]['fields'].get('publications.ti_en') is not None:\n",
    "            \n",
    "            patentName = data[q]['hits'][0]['fields']['publications.ti_en'][0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # searches for title in different lang if no English title found\n",
    "            for w in list(data[q]['hits'][0]['fields']):\n",
    "                if re.search(r'publications.ti',w):\n",
    "                    hasTitle = True\n",
    "                    patentName = data[q]['hits'][0]['fields'][w][0]\n",
    "\n",
    "        \n",
    "        if not hasTitle or not hasPubDate:\n",
    "            # if no title or publication date was found, directly look up the patent info \n",
    "\n",
    "            url = \"https://worldwide.espacenet.com/3.2/rest-services/search/family/\"+familyNumber+\"/aggregated/biblio\"\n",
    "            querystring = {\"lang\":\"en,de,fr\",\"q\":publicationNumber,\"qlang\":\"cql\"}\n",
    "            payload = \"\"\n",
    "            headers = {\n",
    "                \"Accept\": \"multipart/form-data\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Cookie\": \"_pk_id.16.72ee=e201aa9064b4f0dd.1704417755.; _pk_ref.16.72ee=%5B%22%22%2C%22%22%2C1704421700%2C%22https%3A%2F%2Fwww.google.com%2F%22%5D; _pk_ses.16.72ee=1; SameSite=None\",\n",
    "                \"EPO-Trace-Id\": \"iazwu4-e9z380-BBB-000005\",\n",
    "                \"Referer\": \"https://worldwide.espacenet.com/patent/search/family/003734942/publication/ID16772A?q=ID16772A\",\n",
    "                \"Sec-Fetch-Dest\": \"empty\",\n",
    "                \"Sec-Fetch-Mode\": \"cors\",\n",
    "                \"Sec-Fetch-Site\": \"same-origin\",\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "                \"X-EPO-PQL-Profile\": \"cpci\",\n",
    "                \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"104\\\", \\\"Not A;Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"104\\\"\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"sec-ch-ua-platform\": \"\\\"Windows\\\"\"\n",
    "            }\n",
    "\n",
    "            response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "            patInfo = \"\"\n",
    "\n",
    "            try:\n",
    "                patInfo = json.loads(extractJSON(response.text))\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                # if unable to load json, print patent json and error information\n",
    "\n",
    "                print(\"Exception type:\", type(e).__name__)\n",
    "                print(\"\\n\\nError by: \", familyNumber)\n",
    "                print(\"\\n\\nJson Text: \")\n",
    "                print(response.text)\n",
    "                print(\"\\n\\nExtracted Text: \")\n",
    "                print(extractJSON(response.text))\n",
    "                print()\n",
    "\n",
    "            if patInfo != \"\":\n",
    "                if not hasTitle:\n",
    "                    \n",
    "                    if patInfo[applicationNumber].get('ti_en'):\n",
    "                        patentName = list(patInfo[applicationNumber]['ti_en'].values())[0]\n",
    "                        \n",
    "                    elif patInfo[applicationNumber].get('ti_ol'):\n",
    "                        patentName = list(patInfo[applicationNumber]['ti_ol'].values())[0]\n",
    "                            \n",
    "                if not hasPubDate:\n",
    "                    if patInfo[applicationNumber].get('pd'):\n",
    "                        if len(patInfo[applicationNumber]['pd']) == 1:\n",
    "                            publicationDate = patInfo[applicationNumber]['pd'].values()[0]\n",
    "                        else:\n",
    "                            for x in patInfo[applicationNumber]['pd'].keys():\n",
    "                                if x in str(publicationNumber)[-2:]:\n",
    "                                    publicationDate = patInfo[applicationNumber]['pd'][x][0]\n",
    "    \n",
    "        \n",
    "        # extract patent abstract\n",
    "        \n",
    "        if data[q]['hits'][0]['fields'].get('publications.abs_en') is not None:\n",
    "\n",
    "            if re.search(r'<p.*?>(.*?)</p>',data[q]['hits'][0]['fields']['publications.abs_en'][0]) is not None:\n",
    "                patentAbstract = re.search(r'<p.*?>(.*?)</p>', data[q]['hits'][0]['fields']['publications.abs_en'][0]).group(1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # searches for abstract in a different language if no English abstract found\n",
    "\n",
    "            for w in list(data[q]['hits'][0]['fields']):\n",
    "                if re.search(r'publications.abs', w):\n",
    "                    if re.search(r'<p.*?>(.*?)</p>', data[q]['hits'][0]['fields'][w][0]) is not None:\n",
    "                        patentAbstract = re.search(r'<p.*?>(.*?)</p>', data[q]['hits'][0]['fields'][w][0]).group(1)\n",
    "\n",
    "\n",
    "        # extract simple family member application numbers\n",
    "            # defines \"familyMembers\" as list of simple family members (e.g. \"US201514658322A; US43008803A\")\n",
    "\n",
    "        familyMembers = \"\"\n",
    "\n",
    "        if len(list(data[q]['fields']['biblio'][0])) > 0:\n",
    "            for fam in list(data[q]['fields']['biblio'][0]):\n",
    "                if familyMembers == \"\":\n",
    "                    familyMembers = fam\n",
    "                else:\n",
    "                    familyMembers += \"; \"+fam\n",
    "\n",
    "\n",
    "        # calculate number of geographical extentions based off list of family members\n",
    "\n",
    "        geographicalExtensions = len(list(set(map(areaCodeRegex, list(data[q]['fields']['biblio'][0])))))\n",
    "\n",
    "\n",
    "        # extract dates of the oldest and youngest patent in the family\n",
    "            # defines \"familyDates_min\" and \"familyDates_max\" as the earliest and oldest publication dates in the family\n",
    "            # calculates difference in months between the two dates, stored in variable \"timeDifference\"\n",
    "\n",
    "        mindate = dt.date(3000,1,1)\n",
    "        maxdate = dt.date(1,1,1)\n",
    "\n",
    "        for app in data[q]['fields']['biblio'][0]:\n",
    "            for d in data[q]['fields']['biblio'][0][app]['pd'].values():\n",
    "                date = list(map(int, d[0].split(\"-\")))\n",
    "                date = dt.date(date[0],date[1],date[2])\n",
    "                if date < mindate:\n",
    "                    mindate = date\n",
    "                if date > maxdate:\n",
    "                    maxdate = date\n",
    "\n",
    "        familyDates_min = mindate.strftime(\"%Y-%m-%d\")\n",
    "        familyDates_max = maxdate.strftime(\"%Y-%m-%d\")\n",
    "        timeDifference = calculateMonthDiff(familyDates_min, familyDates_max)\n",
    "\n",
    "\n",
    "        # extract ipc classes\n",
    "            # defines \"ipc\" as list of classifications (e.g. C12Q1/68; C12Q1/6827)\n",
    "            # defines \"ipcNum\" as the number of cpc classifications\n",
    "\n",
    "        ipc = \"\"\n",
    "        ipcNum = 0\n",
    "\n",
    "        if data[q]['hits'][0]['fields'].get('publications.ipc_icai') is not None:\n",
    "            for i in data[q]['hits'][0]['fields']['publications.ipc_icai']:\n",
    "                if ipc == \"\":\n",
    "                    ipc = i\n",
    "                else:\n",
    "                    ipc += \"; \"+i\n",
    "\n",
    "                ipcNum += 1\n",
    "\n",
    "\n",
    "        # extract cpc classes\n",
    "            # defines \"cpc\" as string of classifications (e.g. \"C08K13/02 (EP); C08K3/20 (EP)\"\")\n",
    "            # defines \"cpcNum\" as the number of cpc classifications\n",
    "\n",
    "        cpc = \"\"\n",
    "        cpcNum = 0\n",
    "\n",
    "        if data[q]['hits'][0]['fields'].get('publications.ci_cpci') is not None:\n",
    "            for i in data[q]['hits'][0]['fields']['publications.ci_cpci']:\n",
    "                if cpc == \"\":\n",
    "                    cpc = i\n",
    "                else:\n",
    "                    cpc += \"; \"+i\n",
    "\n",
    "                cpcNum += 1\n",
    "\n",
    "        if data[q]['hits'][0]['fields'].get('publications.ca_cpci') is not None:\n",
    "            for i in data[q]['hits'][0]['fields']['publications.ca_cpci']:\n",
    "                if cpc == \"\":\n",
    "                    cpc = i\n",
    "                else:\n",
    "                    cpc += \"; \"+i\n",
    "\n",
    "                cpcNum += 1\n",
    "\n",
    "\n",
    "        # extract applicant names + region\n",
    "            # defines \"applicant\" as a string of applicants (e.g. \"John (US); Paul (HK)\")\n",
    "\n",
    "        applicant = \"\"\n",
    "        \n",
    "        if data[q]['hits'][0]['fields'].get('publications.pa') is not None:\n",
    "            for i in range(len(data[q]['hits'][0]['fields']['publications.pa'])):\n",
    "                app = data[q]['hits'][0]['fields']['publications.pa'][i]\n",
    "                reg = \"\"\n",
    "\n",
    "                if (data[q]['hits'][0]['fields'].get('publications.pac') is not None) and (i < len(data[q]['hits'][0]['fields']['publications.pac'])):\n",
    "                    reg = \" (\"+data[q]['hits'][0]['fields']['publications.pac'][i]+\")\"\n",
    "\n",
    "                if applicant == \"\":    \n",
    "                    applicant = app+reg\n",
    "                else:\n",
    "                    applicant += \"; \"+app+reg\n",
    "\n",
    "\n",
    "        # extract inventor names + region\n",
    "            # defines \"inventor\" as a string of inventors (e.g. \"John (US); Paul (HK)\")\n",
    "\n",
    "        inventor = \"\"\n",
    "\n",
    "        if data[q]['hits'][0]['fields'].get('publications.in') is not None:\n",
    "            for i in range(len(data[q]['hits'][0]['fields']['publications.in'])):\n",
    "                inv = data[q]['hits'][0]['fields']['publications.in'][i]\n",
    "                reg = \"\"\n",
    "\n",
    "                if (data[q]['hits'][0]['fields'].get('publications.inc') is not None) and (i < len(data[q]['hits'][0]['fields']['publications.inc'])):\n",
    "                    reg = \" (\"+data[q]['hits'][0]['fields']['publications.inc'][i]+\")\" \n",
    "\n",
    "                if inventor == \"\":    \n",
    "                    inventor = inv+reg\n",
    "                else:\n",
    "                    inventor += \"; \"+inv+reg\n",
    "\n",
    "\n",
    "        # append info to new row in dataframe\n",
    "\n",
    "        df.loc[q] = [familyNumber, applicationNumber, applicationDate, publicationNumber, publicationDate, patentName, patentAbstract, applicant, inventor, ipc, ipcNum, cpc, cpcNum, familyMembers, geographicalExtensions, familyDates_min, familyDates_max, timeDifference]\n",
    "\n",
    "\n",
    "    # tracks progress\n",
    "    \n",
    "    count += 1\n",
    "    if (int(count/total*100) > int((count-1)/total*100)):\n",
    "        print(\"|\", end=\"\")\n",
    "        \n",
    "        # periodically saves a backup file to data folder\n",
    "        \n",
    "        if q >= len(df)-1:\n",
    "            df.to_csv(os.getcwd()+f'/data/{region}/backup_{region}.csv',index=False,encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new columns to the dataframe to hold citation information. Initialise values to be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "# sort dataframe\n",
    "\n",
    "df = df.sort_values(by=['familyNumber','applicationDate']).drop_duplicates(subset=['familyNumber','applicationNumber','publicationNumber'],keep='last')\n",
    "\n",
    "\n",
    "# initialise new columns for citation data\n",
    "\n",
    "df['backwardCitationsESpaceRaw'] = 0\n",
    "df['backwardCitationsESpaceA'] = 0\n",
    "df['backwardCitationsESpaceB'] = 0\n",
    "\n",
    "df['backwardCitationsGoogle'] = 0\n",
    "df['forwardCitationsGoogle'] = 0\n",
    "df['publicationNumberGoogle'] = \"\"\n",
    "\n",
    "\n",
    "# Saves the dataframe as a csv file in the /data folder. File name can be adjusted below.\n",
    "\n",
    "df.to_csv(os.getcwd()+f'/data/{region}/{startYear}-{endYear}Patents_{region}.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads dataframe from backup file (only run if using a backup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "\n",
    "# df = pd.read_csv(os.getcwd()+f'/data/{region}/backup{region}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find family citations -- Espacenet \n",
    "\n",
    "Defines parameters to be used in API call. No parameters to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "querystring = {\"citationFields\":\"publications.ti_*,publications.ct,publications.pn_docdb,publications.pd,oprid_full.untouched,opubd_full.untouched,publications.abs_*,publications.in,publications.inc,publications.pa,publications.pac,publications.app_fdate.untouched,publications.famn,publications.ci_cpci,publications.ca_cpci,publications.cl_cpci,publications.ipc_icai,publications.ipc_ican\",\n",
    "               \"lang\":\"en,de,fr\",\n",
    "               \"qlang\":\"cql\"}\n",
    "\n",
    "payload = \"\"\n",
    "headers = {\n",
    "    \"Accept\": \"multipart/form-data\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cookie\": \"_pk_id.16.72ee=54208df12fa28cc0.1703812775.; _pk_ses.16.72ee=1; SameSite=None\",\n",
    "    \"EPO-Trace-Id\": \"z09bmc-43bgij-TTT-000088\",\n",
    "    \"Referer\": \"https://worldwide.espacenet.com/patent/search/family/073002675/publication/TW202029089A?f=publications.pac%3Ain%3DHK&q=pd%20within%20%222020-01-01%2C%202020-12-31%22\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"118\\\", \\\"Google Chrome\\\";v=\\\"118\\\", \\\"Not=A?Brand\\\";v=\\\"99\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the number of citations for each patent based on information presented in the Espacenet API. \n",
    "Returns 3 values with different processing methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11\n",
    "\n",
    "# initialise variables with empty values\n",
    "\n",
    "last = \"\"\n",
    "refCountRaw, refCountA, refCountB = None, None, None\n",
    "\n",
    "\n",
    "# set variables for tracking progress\n",
    "\n",
    "total = len(df)\n",
    "count = 0\n",
    "\n",
    "\n",
    "# prints reference bar for tracking progress\n",
    "print(\"|\"*100)\n",
    "\n",
    "# !! adjust start if using backup data. (e.g. if backup file contains ESPacenet citation data until row index 1000, set start to 1000)\n",
    "start = -1\n",
    "\n",
    "\n",
    "# iterate through each patent in dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # index = row number (zero-based indexing), row = series object holding row info\n",
    "    \n",
    "    # skips over rows before \"start\" row (if loading in backup file, remember to adjust \"start\" row)\n",
    "    if index >= start:\n",
    "        \n",
    "        familyNumber = str(row['familyNumber'])\n",
    "        \n",
    "        # reuse last iteration's information if patent is from the same family sa the last patent (citation data identical within families on espacenet)\n",
    "        \n",
    "        if familyNumber != last:\n",
    "            \n",
    "            # API request call\n",
    "            \n",
    "            citations = \"\"\n",
    "            url = \"https://worldwide.espacenet.com/3.2/rest-services/search/family/\"+familyNumber+\"/aggregated/biblio\"\n",
    "            try:\n",
    "                response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "            except:\n",
    "                time.sleep(1) # attempts to catch timeout errors\n",
    "                response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "            \n",
    "            # convert API text string to a json object\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(extractJSON(response.text))\n",
    "                \n",
    "            except Exception as e:\n",
    "                \n",
    "                # if unable to load json, print patent and error information. \n",
    "                # depending on the API call output, regex in function \"extractJSON()\" may need to be adjusted.\n",
    "                \n",
    "                print(\"Exception type:\", type(e).__name__)\n",
    "                print(\"\\n\\nError by: \", familyNumber)\n",
    "                print(\"\\n\\nJson Text: \")\n",
    "                print(response.text)\n",
    "                print(\"\\n\\nExtracted Text: \")\n",
    "                print(extractJSON(response.text))\n",
    "                print()\n",
    "            \n",
    "            # lists to hold the patent citations (\"id_pat\") and citations of non-patent literature (\"id_npl\")\n",
    "            \n",
    "            id_pat = []\n",
    "            id_npl = []\n",
    "            \n",
    "            for item in list(data): \n",
    "                \n",
    "                # id the citation with publication number for patent references\n",
    "                # and normalised reference for non-patent literature\n",
    "                \n",
    "                if \"citation\" in item:\n",
    "                    match = re.match(r'citation\\..*?-(.*)\\..*', item)\n",
    "\n",
    "                    if \"NPL\" in item:\n",
    "                        i_d = normaliseReference(data[item]['ct']['*']['reference'])\n",
    "                        id_npl.append(i_d)\n",
    "                        \n",
    "                    else:\n",
    "                        i_d = match.group(1)\n",
    "                        id_pat.append(i_d)\n",
    "\n",
    "\n",
    "            # RAW: data count from json\n",
    "            \n",
    "            refCountRaw = len(id_pat) + len(id_npl)\n",
    "\n",
    "            # check similarity\n",
    "            \n",
    "            simIndices_pat = []\n",
    "            simIndices_npl = []\n",
    "            id_pat = sorted(id_pat)\n",
    "            id_npl = sorted(id_npl)\n",
    "\n",
    "            for i in range(len(id_pat)):\n",
    "                if i == 0:\n",
    "                    simIndices_pat.append(0)\n",
    "                else:\n",
    "                    simIndices_pat.append(compareReferences(id_pat[i], id_pat[i-1]))\n",
    "\n",
    "            for i in range(len(id_npl)):\n",
    "                if i == 0:\n",
    "                    simIndices_npl.append(0)\n",
    "                else:\n",
    "                    simIndices_npl.append(compareReferences(id_npl[i], id_npl[i-1])) \n",
    "\n",
    "            # PRPCESS A: count citations after removing duplicate items with 100% similarity\n",
    "            \n",
    "            simIndicesA = [x for x in simIndices_pat if int(x) < 1] + [x for x in simIndices_npl if float(x) < 1]\n",
    "            refCountA = len(simIndicesA)\n",
    "\n",
    "            # PROCESS B: count citations after removing NPL references with over 95% similarity, and patent references with 100% similarity\n",
    "            \n",
    "            simIndicesB = [x for x in simIndices_pat if int(x) < 1] + [x for x in simIndices_npl if float(x) < 0.95]\n",
    "            refCountB = len(simIndicesB)\n",
    "        \n",
    "        # update dataframe with calculated counts\n",
    "        \n",
    "        df.loc[index, 'backwardCitationsESpaceRaw'] = refCountRaw\n",
    "        df.loc[index, 'backwardCitationsESpaceA'] = refCountA\n",
    "        df.loc[index, 'backwardCitationsESpaceB'] = refCountB\n",
    "\n",
    "        last = familyNumber  \n",
    "    \n",
    "    # tracks progress\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if (int(count/total*100) > int((count-1)/total*100)):\n",
    "        \n",
    "        print(\"|\", end=\"\")\n",
    "        \n",
    "        if index >= start:\n",
    "            # periodically saves backup file\n",
    "            df.to_csv(os.getcwd()+f'/data/{region}/backup_{region}.csv',index=False,encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find family citations -- GooglePatents\n",
    "\n",
    "Defines parameters to be used in API call. No parameters to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "payload = \"\"\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cookie\": \"AEC=Ackid1RnDVfJgNrOQbsJ-QvTlEFUBOFC1vy5l_VNxJfe3rYQVoOXuJonQIE; SID=ewjm5qMzOWKmTVjcF4ljDJ_Qjpa_VeZjBZZiYsL4YzU4zo418lqG3brekZR4-qj7fwTA5w.; __Secure-1PSID=ewjm5qMzOWKmTVjcF4ljDJ_Qjpa_VeZjBZZiYsL4YzU4zo41wqVyi148PMZsEQ9p0PeBDg.; __Secure-3PSID=ewjm5qMzOWKmTVjcF4ljDJ_Qjpa_VeZjBZZiYsL4YzU4zo41a4daAgzcdUc6V_3IkYoUVA.; HSID=A-NEN_B7y29vlurDW; SSID=ACq_HLV2Xi2TvGb1X; APISID=sgxNCGTS5IkotwRV/Aj48pz6c5j7BL2j5l; SAPISID=lMzSbV9Y5wi16KgA/AI1tD5whIm7rxQ0Ay; __Secure-1PAPISID=lMzSbV9Y5wi16KgA/AI1tD5whIm7rxQ0Ay; __Secure-3PAPISID=lMzSbV9Y5wi16KgA/AI1tD5whIm7rxQ0Ay; SEARCH_SAMESITE=CgQIjJoB; 1P_JAR=2024-01-02-06; NID=511=IMfcdhimmvf78f-GOe-jdouV4Z1gdEnemPMiZvnK80a7GUwUaBQXVmqttEIM4KRZ-yJ3xn6TaPD8pI8SPh4rAR0NIQpDifg3wdMqiqxoTOI3YPMs7FiiSQIoh1USlSCL9DdV4zWRxz1VCJ4veAI3kLmcJAkk66Wdbt-NWj6GvOttKng_Gr0I_u3ddvJmYfJW-VcdHSKEgWosRL_omb6PAzKDnofG9XD3BoLoYTSFfgjFjNc43ByUx0nEsUBJxTVVDW1iO-da86V8IqVFF_uJ2wAz3hsSXw3J0Z6fajVwa0PXWDS7e25H4cQQtF9l0AFWn4S3-TeuIcHQ; SIDCC=ABTWhQFIcVBrzBklfqYryiP1olTqMdDgMfKDrbYmjKfVmGOGcvImSeA57yptRpdK0S3QqdytpQ; __Secure-1PSIDCC=ABTWhQHx2PbNYCEnL5oO-78T6uyoaMjUUyKnQ6vgC9o5JWVsb-zNAOJUFHgs_odEtuSIGkci; __Secure-3PSIDCC=ABTWhQEoHLQSDx_AhlLyv-MTeWNXOy6hi_B0CTBawmGLQatMQhHWAG__eTIH6rvzwx-EN2hP\",\n",
    "    \"Referer\": \"https://patents.google.com/\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"X-Client-Data\": \"CK6LywE=\",\n",
    "    \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"118\\\", \\\"Google Chrome\\\";v=\\\"118\\\", \\\"Not=A?Brand\\\";v=\\\"99\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the number of backward and forward citations for each patent based on information presented in the Google Patents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13\n",
    "\n",
    "# set variables for tracking progress\n",
    "\n",
    "total = len(df)\n",
    "count = 0\n",
    "\n",
    "# prints reference bar for tracking progress\n",
    "\n",
    "print(\"|\"*100)\n",
    "\n",
    "# adjust start if using backup data. (e.g. if backup file contains Google Patent citation data until row index 1000, set start to 1000)\n",
    "\n",
    "start = -1\n",
    "\n",
    "# iterate through each patent in dataframe\n",
    "    # index = row number (zero-based indexing), row = series object holding row info\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    if index >= start:\n",
    "        publicationNumber = row['publicationNumber']\n",
    "        querystring = {\"id\":\"patent/\"+str(publicationNumber)+\"/en\"}\n",
    "        url = \"https://patents.google.com/xhr/result\"\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "            \n",
    "        # if patent can't be found on Google Patents\n",
    "\n",
    "        if response.status_code == 404:\n",
    "\n",
    "            # find similar publication number through google patents search bar\n",
    "\n",
    "            url404 = \"https://patents.google.com/xhr/parse\"\n",
    "            querystring404 = {\"text\":publicationNumber}\n",
    "            \n",
    "            try:\n",
    "                response404 = requests.request(\"GET\", url404, data=payload, headers=headers, params=querystring404)\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "                response404 = requests.request(\"GET\", url404, data=payload, headers=headers, params=querystring404)\n",
    "                \n",
    "            searchResult = response404.json()\n",
    "\n",
    "           # find top search result, store new publication number in new column\n",
    "\n",
    "            if not searchResult['error_no_patents_found']:\n",
    "\n",
    "                if searchResult['results'][0].get('result'):\n",
    "                    publicationNumber = searchResult['results'][0]['result']['number']   \n",
    "\n",
    "                else:\n",
    "                    url = \"https://patents.google.com/xhr/query\"\n",
    "                    querystring = {\"url\":f\"q=({publicationNumber})&oq={publicationNumber}\",\"exp\":\"\",\"tags\":\"\"}\n",
    "\n",
    "                    try:\n",
    "                        searchResult = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring).json()\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                        searchResult = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring).json()\n",
    "\n",
    "                    publicationNumber = searchResult['results']['cluster'][0]['result'][0]['patent']['publication_number']\n",
    "\n",
    "                df.loc[index,'publicationNumberGoogle'] = publicationNumber\n",
    "\n",
    "                # re-instantiate querystring and response with new publication number\n",
    "\n",
    "                querystring = {\"id\":\"patent/\"+str(publicationNumber)+\"/en\"}\n",
    "                try:\n",
    "                    response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "                    response = requests.request(\"GET\", url, data=payload, headers=headers, params=querystring)\n",
    "                    \n",
    "\n",
    "        # create beautiful soup object to parse html data\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "\n",
    "        # calculate number of backward citations\n",
    "\n",
    "        origBackCitations = soup.find_all('tr', itemprop='backwardReferencesOrig')\n",
    "        famBackCitations = soup.find_all('tr', itemprop='backwardReferencesFamily')\n",
    "        nplBackCitations = soup.find_all('tr', itemprop='detailedNonPatentLiterature')\n",
    "        df.loc[index, 'backwardCitationsGoogle'] = len(origBackCitations) + len(famBackCitations) + len(nplBackCitations)\n",
    "        \n",
    "\n",
    "        # calculate number of forward citations\n",
    "\n",
    "        origFwdCitations = soup.find_all('tr', itemprop='forwardReferencesOrig')\n",
    "        famFwdCitations = soup.find_all('tr', itemprop='forwardReferencesFamily')\n",
    "        df.loc[index, 'forwardCitationsGoogle'] = len(origFwdCitations) + len(famFwdCitations)\n",
    "\n",
    "\n",
    "    # tracks progress\n",
    "    \n",
    "    count += 1\n",
    "    if (int(count/total*100) > int((count-1)/total*100)):\n",
    "        print(\"|\", end=\"\")\n",
    "        \n",
    "        if index >= start:\n",
    "            \n",
    "            # periodically saves backup file\n",
    "            df.to_csv(os.getcwd()+f'/data/{region}/backup_{region}.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the dataframe as a csv file in the `/data/REGION` folder. File name can be adjusted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "\n",
    "df.to_csv(os.getcwd()+f'/data/{region}/{startYear}-{endYear}PatentsWithCitations_{region}.csv',index=False,encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
